---
layout: mypost
title: 吴恩达深度学习课程笔记
categories: [笔记, 深度学习]
---

* awsl
{:toc}

# 第一课

待补充

# 第二课

## 2.1 深度学习的实用层面

### 2.1.1 训练&验证&测试集

小数据：60+20+20

大数据：98+1+1 or 99.5 + 0.25 +0.25

<font color=008080> 2021/1/25 P2 </font>

### 2.1.2 偏差(Bias)&方差(Variance)

![](https://i.loli.net/2021/01/26/eKphULNQA9SyRcJ.png)

|                 | high variance | high bias | high bias & high variance | low bias & low variance |
| --------------- | ------------- | --------- | ------------------------- | ----------------------- |
| Train set error | 1%            | 15%       | 15%                       | 0.5%                    |
| Dev set error   | 11%           | 16%       | 30%                       | 1%                      |

###  2.1.3 机器学习的基础"recipe"

![](https://i.loli.net/2021/01/26/HhoMnjJZpLiSQ79.png)

### 2.1.4 正则化（Regularization）

- L1正则化
- L2正则化

### 2.1.5 为什么正则化可以减少过拟合（overfitting）

### 2.1.6 Dropout Regularization

Dropout : 依照概率（keep-prob）随机删除网络中的神经元

![](https://i.loli.net/2021/01/26/lmpdz2UIvLYN74b.png)

- Inverted dropout : /= keep-prob 保持期望相同

### 2.1.7 理解 Dropout

- Can't rely on any one feature, so have to spread out weights.
- 常用于CV领域：样本少，过拟合很常见

### 2.1.8 其他正则化方法

- Data augmentation(数据扩增): 水平翻转，随机裁剪等

- Early stopping: 提早停止训练神经网络

### 2.1.9 归一化输入（Normalizing inputs）

![](https://i.loli.net/2021/01/26/8rtQKTonObWRUhv.png)

1. 零均值化：
   $$
   x:=x-μ
   $$

2. 归一化方差

$$
x:=x/σ^2
$$

### 2.1.10 梯度消失/爆炸（Vanishing/exploding gradients）

神经层数很多时容易发生。

