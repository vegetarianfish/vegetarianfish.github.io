---
layout: mypost
title: 机器学习笔记
categories: [note, machine-learning]
---

# 机器学习笔记

> 为了考研复试重温一下
>
> 只是简单梳理，详情参考PPT和西瓜书

* awsl
{:toc}
## 第一章 绪论

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-01%E7%BB%AA%E8%AE%BA.pdf)

### - 基本术语

- 数据（对象）

- 任务

  - 按预测目标
    - 分类：离散值
    - 回归：连续值
    - 聚类：无标记信息
  - 按有无标记信息
    - 监督学习：分类、回归
    - 无监督学习：聚类
    - 半监督学习：两者结合
    - 扩展：弱监督学习、自监督学习

- 泛化能力（目标）

  机器学习的目标是使得学到的模型能很好的适用于“新样本”，而不仅仅是训练集合，我们称模型适用于新样本的能力为**泛化（generalization）能力**。

### - 假设空间

### - 归纳偏好

- 学习过程中对某种类型假设的偏好称作归纳偏好

- 归纳偏好可看作学习算法自身在一个可能很庞大的假 设空间中对假设进行选择的启发式或“价值观”。

- “奥卡姆剃刀”是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，选最简单的那个”。

- 具体的现实问题中，学习算法本身所做的假设是否成立，也即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。

- NoFreeLunch

  一个算法a如果在某些问题上比另一个算法b好，必然存在另一些问题，b比a好,也即没有免费的午餐定理。

### - 发展历程

- 推理期
- 知识期
- 学习期

![](https://i.loli.net/2021/02/04/LybSXaZVi1AzwK6.png)

### - 应用现状

## 第二章 模型评估与选择

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-02%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9.pdf)

### - 经验误差与过拟合

- 过拟合

  学习器把训练样本学习的“太好”，将训练样本本身的特点当做所有样本的一般性质，导致泛化性能下降

  - 优化目标加正则项
  - early stop
  
- 欠拟合：

  对训练样本的一般性质尚未学好

  - 决策树：拓展分支
  - 神经网络：增加训练轮数

### - 评估方法

- 评估考虑的主要因素

  泛化性能、时间开销、存储开销、可解释性等方面

- 评估样本选取原则

  我们假设测试集是从样本真实分布中独立采样获得，将测试集上的“测试误差”作为泛化误差的近似，所以测试集要和训练集中的样本互斥。

- 留出法(hold-out)

  - 直接将数据集划分为两个互斥集合
  - 训练/测试集划分要尽可能保持数据分布的一致性
  - 一般若干次随机划分、重复实验取平均值
  - 训练/测试样本比例通常为2:1至4:1

- 交叉验证法(cross validation)

  将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值，k最常用的取值是10

- 自助法(bootstrapping)

  - 实际模型与预期模型都使用m个训练样本
  - 约有1/3的样本没在训练集中出现
  - 从初始数据集中产生多个不同的训练集，对集成学习有很大的好处
  - 自助法在数据集较小、难以有效划分训练/测试集时很有用；由于改变了数据集分布可能引入估计偏差，在数据量足够时，留出法和交叉验证法更常用。

### - 性能度量

- 性能度量是衡量模型泛化能力的评价标准，反映了任务 需求；使用不同的性能度量往往会导致不同的评判结果

- 回归任务最常用的性能度量是“**均方误差**”

- 对于分类任务，**错误率**和**精度**是最常用的两种性能度量

- 混淆矩阵

  ![](https://i.loli.net/2021/02/04/Bh8npWomef9QNdk.png)

- P-R曲线

- F1度量，$F_\beta$度量

- ROC曲线

- 代价敏感错误率

### - 比较检验

- 二项检验
- t检验
- 交叉验证t检验
- 5×2交叉验证法

### - 偏差与方差

- **偏差**度量了学习算法期望预测与真实结果的偏离程度；即刻画了学习算法本身的拟合能力
- **方差**度量了同样大小训练集的变动所导致的学习性能的变化； 即刻画了数据扰动所造成的影响
- **噪声**表达了在当前任务上任何学习算法所能达到的期望泛化 误差的下界；即刻画了学习问题本身的难度。
- 一般来说，偏差与方差是有冲突的，称为偏差-方差窘境。 如图所示，假如我们能控制算法的训练程度
  - 在训练不足时，学习器拟合能力不强，训练数据的扰动不足以使学习器的拟合能力产生显著变化，此时偏差主导泛化错误率
  - 随着训练程度加深，学习器拟合能力逐渐增强，方差逐渐主导泛化错误率
  - 训练充足后，学习器的拟合能力非常强，训练数据的轻微扰动都会导致学习器的显著变化，若训练数据自身非全局特性被学到则会发生过拟合

![](https://i.loli.net/2021/02/04/LS97TdHgvGVzbsD.png)

## 第三章 线性模型

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-03%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.pdf)

### - 基本形式

$$
f(x)=\boldsymbol{w}^T\boldsymbol{x}+b
$$

### - 线性模型优点

- 形式简单、易于建模
- 可解释性
- 非线性模型的基础：在线性模型基础上，引入层级结构或高维映射，实现非线性

### - 线性回归

- 参数/模型估计：最小二乘法（least square method）

  最小二乘法是一种数学优化技术，通过最小化误差的平方和寻找数据的最佳函数匹 配

- 梯度下降

### - 二分类任务

- 对数几率回归-极大似然法

- 线性判别分析（Linear Discriminant Analysis）

  LDA的思想

  - 欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小 
  -  欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大

###  - 多分类学习

- 多分类学习方法
  - 二分类学习方法推广到多类
  - 利用二分类学习器解决多分类问题（常用）
    - 对问题进行拆分，为拆出的每个二分类任务训练一个分类器
    - 对于每个分类器的预测结果进行集成以获得最终的多分类结果
- 拆分策略
  - 一对一（One vs. One, OvO）
  - 一对其余（One vs. Rest, OvR）
  - 多对多（Many vs. Many, MvM）

### - 优化提要

最优化是应用数学的一个分支，主要指在一定条件限制下，选取某种研究方案使 目标达到最优的一种方法。

- 各任务下（回归、分类）各个模型优化的目标

  - 最小二乘法：最小化均方误差

  - 对数几率回归（逻辑回归）：最大化样本分布似然

  - 线性判别分析：投影空间内最小（大）化类内（间）散度 

- 参数的优化方法

  - 最小二乘法：线性代数
  
- 对数几率回归：凸优化梯度下降、牛顿法
  - 线性判别分析：矩阵论、广义瑞利商

## 第四章 决策树

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-04%E5%86%B3%E7%AD%96%E6%A0%91.pdf)

### - 基本流程

- 决策过程中提出的每个判定问题都是对某个属性的“测试”

- 决策过程的最终结论对应了我们所希望的判定结果 

- 每个测试的结果或是导出最终结论，或是导出进一步的判定问题，其考虑范围在上次决策结果的限定范围之内 

- 从根结点到每个叶结点的路径对应了一个判定测试序列

- <font color='red'>决策树学习的目的是为了产生一棵泛化能力强的决策树即处理未见示例能力强的决策树 </font>

### - 划分选择

决策树学习的关键在于如何选择最优划分属性。一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别， 即结点的“纯度”(purity) 越来越高

经典的属性划分方法：

- 信息增益：信息增益对可取值数目较多的属性有所偏好
- 增益率：增益率准则对可取值数目较少的属性有所偏好
- 基尼指数：反映了从 $D$ 中随机抽取两个样本，其类别标记不一致的概率

### - 剪枝处理

- 为什么剪枝

  - “剪枝”是决策树学习算法对付“过拟合”的主要手段
  - 可通过“剪枝”来一定程度避免因决策分支过多，以致于把训练集自身的一些特点当做所有数据都具有的一般性质而导致的过拟合

- 剪枝的基本策略

  - 预剪枝
  - 后剪枝

- 判断决策树泛化性能是否提升的方法

  - 留出法：预留一部分数据用作“验证集”以进行性能评估

- 预剪枝

  - 决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分 不能带来决策树泛化性能提升，则停止划分并将当前结点记为叶结点，其 类别标记为训练样例数最多的类别

  - 针对上述数据集，基于信息增益准则，选取属性“脐部”划分训练集。分别计算划分前（即直接将该结点作为叶结点）及划分后的验证集精度，判断是否需要划分。若划分后能提高验证集精度，则划分，对划分后的属性，执行同样判断；否则，不划分

  - 优点

    - 降低过拟合风险
  - 显著减少训练时间和测试时间开销 
  
  - 缺点
    - 欠拟合风险：有些分支的当前划分虽然不能提升泛化性能，但在其基础上进行的后 续划分却有可能导致性能显著提高。预剪枝基于“贪心（在问题求解时，总做出在当前看来最好的选择）”本质禁止这些分支展开， 带来了欠拟合风险
  
- 后剪枝

  先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点

  - 优点 

    后剪枝比预剪枝保留了更多的分支，欠拟合风险小，泛化性能往往优于预剪枝决策树 

  - 缺点

    **训练时间开销大**：后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察
  

### - 连续与缺失值

- 连续值处理：连续属性离散化（二分法）

- 缺失值处理

  - 不完整样本，即样本的属性值缺失

  - 仅使用无缺失的样本进行学习? 

    对数据信息极大的浪费

  - 使用有缺失值的样本，需要解决哪些问题？

    Q1：如何在属性缺失的情况下进行划分属性选择？ 

    Q2：给定划分属性,若样本在该属性上的值缺失， 如何对样本进行划分？

### - 多变量决策树

## 第五章 神经网络

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-05%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.pdf)

### - 神经网络发展史

第一阶段

- 1943年, McCulloch和Pitts 提出第一个神经元数学模型，即M-P模型，并从原 理上证明了人工神经网络能够计算任何算数和逻辑函数
- 1949年, Hebb 发表《The Organization of Behavior》一书, 提出生物神经元学 习的机理，即Hebb学习规则
- 1958年，Rosenblatt 提出感知机网络（Perceptron）模型和其学习规则 
- 1960年，Widrow和Hoff提出自适应线性神经元（Adaline）模型和最小均方 学习算法 
- 1969年，Minsky和Papert 发表《Perceptrons》一书，指出单层神经网路不能解决非线性问题，多层网络的训练算法尚无希望。这个论断导致神经网络进入低谷

第二阶段

- 1982年，物理学家Hopfield提出了一种具有联想记忆、优化计算能力的递归网络模型，即Hopfield 网络
- 1986年，Rumelhart 等编辑的著作《Parallel Distributed Proceesing: Explorations in the Microstructures of Cognition》报告了反向传播算法
- 1987年，IEEE 在美国加州圣地亚哥召开第一届神经网络国际会议（ICNN）
- 90年代初，伴随统计学习理论和SVM的兴起，神经网络由于理论不够清楚、试错性强、难以训练，再次进入低谷

第三阶段

- 2006年，Hinton提出了深度信念网络(DBN)，通过“预训练+微调”使得深度模型的最优 化变得相对容易
- 2012年，Hinton 组参加ImageNet 竞赛，使用 CNN 模型以超过第二名10个百分点的成绩 夺得当年竞赛的冠军 
- 伴随云计算、大数据时代的到来，计算能力的大幅提升，使得深度学习模型在计算机视 觉、自然语言处理、语音识别等众多领域都取得了较大的成功

### - 神经元模型

- 输入：来自其他n个神经云传递过来的输入信号
- 处理：输入信号通过带权重的连接进行 传递，神经元接受到总输入值将与神 元的阈值进行比较 
- 输出：通过激活函数的处理以得到输出

### - 感知机与多层网络

- 感知机由两层神经元组成，输入层接受外界输入信号传递给输出层，输出层是M-P神经 元（阈值逻辑单元）
- 感知机能够容易地实现逻辑与、或、非运算 （阶跃激活）
- 多层前馈神经网络
  - 定义：每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接
  - 前馈：输入层接受外界输入，隐含层与输出层神经元对信号进行加工，最终结果由输出层 神经元输出
  - 学习：根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的“阈值” 多层网络：包含隐层的网络

### - 误差逆传播算法

- 误差逆传播算法（Error BackPropagation，简称BP）是最成功的训练多层前馈神经网络的学习算法
- 多层前馈网络表示能力
  - 只需要一个包含足够多神经元的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数 [Hornik et al. , 1989] 
- 多层前馈网络局限
  - 神经网络由于强大的表示能力, 经常遭遇过拟合。表现为：训练误差持续降低, 但测试误差却可能上升
  - 如何设置隐层神经元的个数仍然是个未决问题。实际应用中通常使用“试错法”调整
- 缓解过拟合的策略
  - 早停：在训练过程中, 若训练误差降低, 但验证误差升高, 则停止训练 
  - 正则化：在误差目标函数中增加一项描述网络复杂程度的部分, 例如连接权值与阈值的平方和

### - 全局最小与局部极小

- “跳出”局部最小的策略

  基于梯度的搜索是使用最为广泛的参数寻优方法。如果误差函数仅有一个局部极小，那 么此时找到的局部极小就是全局最小；然而，如果误差函数具有多个局部极小，则不能 保证找到的解是全局最小。在现实任务中，通常采用以下策略“跳出”局部极小，从而 进一步达到全局最小

  - 多组**不同的初始参数**优化神经网络，选取误差最小的解作为最终参数。
  - **模拟退火技术** [Aarts and Korst, 1989]。 每一步都以一定的概率接受比当前解更差的结果，从 而有助于跳出局部极小。
  - **随机梯度下降**。与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加 入了随机因素。
  - **遗传算法** [Goldberg, 1989]。 遗传算法也常用来训练神经网络以更好地逼近全局极小。

### - 其他常见神经网络

- RBF 网络
- ART 网络（自适应谐振理论网络）
- SOM 网络（自组织映射网络）
- 级联相关网络
- Elman 网络
- Boltzmann 机
- 受限 Boltzmann 机 

### - 深度学习

- 深度学习（Deep Learning）是在2006年由Geoffrey Hinton等人提出来的。 它通过神经网络来模拟人类感知的方法，通过最低层的局部抽象特征，每层 接收下层的输出作为本层的输入，自下而上逐层地构建一个多层的网络来使 得机器能自动地学习到隐含在数据（图像、语音及文本等）内部的关系，最 后获取对目标整体的感知，使得学习到的特征更具有推广性和表达力。

- 深度学习模型
  - 典型的深度学习模型就是很深层的神经网络
  
  - 模型复杂度
    - 增加隐层神经元的数目（模型宽度）
    - 增加隐层数目（模型深度）
    - 从增加模型复杂度的角度看，增加隐层的数目比增加隐层神经元的数目更有效。这是 因为增加隐层数不仅增加了拥有激活函数的神经元数目，还增加了激活函数嵌套的层数。 
    
  - 复杂模型难点
  
    多隐层网络难以直接用经典算法（例如标准BP算法）进行训练，因为误差在多隐层内逆传播时，往往会“发散” 而不能收敛到稳定状态。

- 复杂模型训练方法

  **预训练+微调**

  - 预训练：监督逐层训练是多隐层网络训练的有效手段，每次训练一层隐层结点，训练时将 上一层隐层结点的输出作为输入，而本层隐结点的输出作为下一层隐结点的输入，这称为 “预训练”。
  - 微调：在预训练全部完成后，再对整个网络进行微调训练。微调一般使用BP算法。 例子：深度信念网络 [Hintonet al. , 2006]
  - 结构：每一层都是一个受限 Boltzmann 机
  - 训练方法：无监督预训练+BP 微调 
  
  **分析**
  
  - 预训练+微调的做法可以视为将大量参数分组，对每组先找到局部看起来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优。
  
  **权共享**
  
  - 一组神经元使用相同的连接权值
  - 权共享策略在卷积神经网络（ CNN ）[LeCun and Bengio, 1995; LeCun et al. , 1998]中发挥了重要作用

## 第六章 支持向量机

> [PPT](file:///D:/_study/考研复试/专业课复习/MechineLearning/FML-06支持向量机.pdf)

### - 间隔与支持向量

![](https://i.loli.net/2021/02/05/H8MCybORKalPz5n.png)

### - 对偶问题

### - 核函数

### - 软间隔与正则化

Q：现实中，很难确定合适的核函数使得训练样本在特征空间中线性可分；同时一 个线性可分的结果也很难断定是否是由过拟合造成的。

A：引入“软间隔”的概念，允许支持向量机在一些样本上不满足约束。

![](https://i.loli.net/2021/02/05/bYEjp8GO5cCdxFn.png)

### - 支持向量回归

### - 核方法

## 第七章 贝叶斯分类器

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-07%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.pdf)

### - 贝叶斯决策论

贝叶斯决策论（Bayesian decision theory）是在概率框架下实施决策的基本方法。

对分类问题，在所有相关概率都已知的理想情形下，贝叶斯决策考虑如何基于这些概率和误判损失来选择最优的类别标记。

- 生成式模型
- 判别式模型

### - 极大似然估计

估计类条件概率的常用策略：先假定其具有某种确定的概率分布形式，再基于训练样本对 概率分布参数估计。

概率模型的训练过程就是参数估计过程，统计学界的两个学派提供了不同的方案：

- 频率主义学派 (Frequentist)认为参数虽然未知，但却存在客观值，因此可通过优化似然 函数等准则来确定参数值
- 贝叶斯学派 (Bayesian)认为参数是未观察到的随机变量、其本身也可有分布，因此可假 定参数服从一个先验分布，然后基于观测到的数据计算参数的后验分布。

### - 朴素贝叶斯分类器

- 估计后验概率主要困难：类条件概率 是所有属性上的联合概率，难以从有限的训练样本估计获得。 

- 朴素贝叶斯分类器(Naïve Bayes Classifier)采用了“**属性条件独立性假设**”(attribute conditional independence assumption)：每个属性独立地对分类结果发生影响。

- 拉普拉斯修正

  若某个属性值在训练集中没有与某个类同时出现过，则直接计算会出现问题。比如“敲声 =清脆”测试例，训练集中没有该样例，因此连乘式计算的概率值为0，无论其他属性上明显像好瓜，分类结果都是“好瓜=否”，这显然不合理。（大数定理的条件不成立） 

  为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值时通常 要进行“拉普拉斯修正”（Laplacian correction）

### - 半朴素贝叶斯分类器

- 为了降低贝叶斯公式中估计后验概率的困难，朴素贝叶斯分类器采用的属性条件独立性假 设；对属性条件独立假设进行一定程度的放松，由此产生了一类称为“半朴素贝叶斯分类 器” (semi-naïve Bayes classifiers)的学习方法
- 半朴素贝叶斯分类器最常用的一种策略：“独依赖估计”(One-Dependent Estimator,简称 ODE)，假设每个属性在类别之外最多仅依赖一个其他属性
- 最直接的做法是假设所有属性都依赖于同一属性，称为“超父” (superparent)，然后通过交叉验证等模型选择方法来确定超父属性，由此形成了 SPODE (Super-Parent ODE)方法。
- TAN (Tree augmented Naïve Bayes) [Friedman et al., 1997] 则在最大带 权生成树 (Maximum weighted spanning tree) 算法 [Chow and Liu, 1968] 的基础上，通过以下步骤将属性间依赖关系简约
- AODE (Averaged One-Dependent Estimator) [Webb et al. 2005] 是 一种基于集成学习机制、更为强大的分类器。
  - 尝试将每个属性作为超父构建 SPODE
  - 将具有足够训练数据支撑的SPODE集群起来作为最终结果

### - 贝叶斯网

- 贝叶斯网 (Bayesian network)亦称“信念网”(brief network)，它借助有向无环图 (Directed Acyclic Graph, DAG)来刻画属性间的依赖关系，并使用条件概率表 (Conditional Probability Table, CPT)来表述属性的联合概率分布。

![](https://i.loli.net/2021/02/05/DPeVXkS4z5wLOGx.png)

- 从网络图结构可以看出 -> “色泽”直接依赖于“好瓜”和“甜度” 
- 从条件概率表可以得到 -> “根蒂”对“甜度”的量化依赖关系 P(根蒂＝硬挺|甜度＝高) ＝0.1

### - EM算法

- EM (Expectation-Maximization)算法 [Dempster et al., 1977] 是常用的估 计参数隐变量的利器。
- EM算法用于含有隐变量的概率模型参数的极大似然估计，可求解k-means聚类、 混合高斯模型、隐马尔科夫模型等
- 交替迭代思想
- 迭代初值对参数估计结果有影响
- 实际问题的统计建模

## 第八章 集成学习

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-08%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.pdf)

### - 个体与集成

集成学习(ensemble learning)通过构建并结合多个学习器来提升性能

![](https://i.loli.net/2021/02/05/kWheSwGOi2z43yt.png)

上面的分析有一个关键假设：基学习器的误差相互独立。现实任务中，个体学习器是为解决同一个问题训练出来的，显然不可能互相独立。事实上，个体学习器的“准确性”和“多样性”本身就存在冲突。如何产生“好而不同”的个体学习器是集成学习研究的核心。

集成学习大致可分为两大类：**序列化方法和并行化方法**

### - Boosting

![](https://i.loli.net/2021/02/05/tcbmwTv8n43FxOX.png)

- Adaboost

### - Bagging与随机森林

- 个体学习器不存在强依赖关系
- 并行化生成
- 自助采样法

### - 结合策略

- 平均法
  - 简单平均法是加权平均法的特例
  - 加权平均法在二十世纪五十年代被广泛使用
  - 集成学习中的各种结合方法都可以看成是加权平均法的变种或特例
  - 加权平均法可认为是集成学习研究的基本出发点
  - 加权平均法未必一定优于简单平均法

- 投票法

  - 绝对多数投票法（majority voting）
  - 相对多数投票法（plurality voting）
  - 加权投票法（weighted voting）

- 学习法

  Stacking是学习法的典型代表

### - 多样性

- 误差-分歧分解

- 多样性度量

- 多样性扰动

## 第九章 聚类

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-09%E8%81%9A%E7%B1%BB.pdf)

### - 聚类任务

-  在“无监督学习”任务中研究最多、应用最广

-  聚类目标：将数据集中的样本划分为若干个通常不相交的子集（“簇”， cluster）

- 聚类既可以作为一个单独过程（用于找寻数据内在的分布结构）也可作为分 类等其他学习任务的前驱过程

- 形式化描述

  ![](https://i.loli.net/2021/02/06/7AykDwFzH6po4cd.png)

### - 性能度量

- 聚类性能度量，亦称为聚类“有效性指标”（validity index）
- 直观来讲： 我们希望“物以类聚”，即同一簇的样本尽可能彼此相似，不同簇的样本尽可能不同。换言之，聚类结果的“簇内相似度”（intra-cluster similarity）高，且“簇间相似度”（inter-cluster similarity）低，这样的聚类效果较好.
- 聚类性能度量：
  - 外部指标 (external index) 将聚类结果与某个“参考模型”(reference model)进行比较。
    - Jaccard系数（Jaccard Coefficient, JC）
    - FM指数（Fowlkes and Mallows Index, FMI）
    - Rand指数（Rand Index, RI）
  - 内部指标 (internal index) 直接考察聚类结果而不用任何参考模型。
    - DB指数（Davies-Bouldin Index, DBI）
    - Dunn指数（Dunn Index, DI）

### - 距离计算

- 距离度量的性质：

  ![](https://i.loli.net/2021/02/06/M6jdmTpVEgFP8LG.png)

- 常用距离：

  ![](https://i.loli.net/2021/02/06/HTpJMGuVelit7vm.png)

- 属性介绍

  - 连续属性 (continuous attribute) 在定义域上有无穷多个可能的取值
  - 离散属性 (categorical attribute) 在定义域上是有限个可能的取值
  - 有序属性 (ordinal attribute) 例如定义域为{1,2,3}的离散属性，“1”与“2”比较接近、与 “3”比较远，称为“有序属性”。
  - 无序属性 (non-ordinal attribute) 例如定义域为{飞机，火车，轮船}这样的离散属性，不能直接在属 性值上进行计算，称为“无序属性”。

### - 原型聚类

- 原型聚类

  也称为“基于原型的聚类” (prototype-based clustering)，此类算法假设聚类结构能通过一组原型刻画。

- 算法过程

  通常情况下，算法先对原型进行初始化，再对原型进行迭代更新求解。

- 接下来，介绍几种著名的原型聚类算法 

  k均值算法、学习向量量化算法、高斯混合聚类算法。

### - 密度聚类

- 密度聚类的定义 

  密度聚类也称为“基于密度的聚类” (density-based clustering)。 

  此类算法假设聚类结构能通过样本分布的紧密程度来确定。 

  通常情况下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇来获得最终的聚类结果。 

  接下来介绍DBSCAN这一密度聚类算法。

### - 层次聚类

- 层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚 类结构。数据集划分既可采用“自底向上”的聚合策略，也可采用 “自顶向下”的分拆策略

## 第十章 降维与度量学习

> [PPT](file:///D:/_study/%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0/MechineLearning/FML-10%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0.pdf)

### - k近邻学习

- k近邻(k-Nearest Neighbor, kNN)学习是一种常用的监督学习方法:
  - 确定训练样本，以及某种距离度量。
  - 对于某个给定的测试样本，找到训练集中距离最近的k个样本，对于 分类问题使用“投票法”获得预测结果，对于回归问题使用“平均法” 获得预测结果。还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大
    - 投票法：选择这k个样本中出现最多的类别标记作为预测结果
    - 平均法：将这k个样本的实值输出标记的平均值作为预测结果。

- K近邻学习没有显式的训练过程，属于“懒惰学习”
  - “懒惰学习”(lazy learning): 此类学习技术在训练阶段仅仅是 把样本保存起来，训练时间开销为零，待收到测试样本后再进行处 理。
  - “急切学习”(eager learning): 在训练阶段就对样本进行学习处理的方法。
- k近邻分类器中的k是一个重要参数，当k取不同值时，分类结果会有显著不同。另一方面，若采用不同的距离计算方式，则找出的 “近邻”可能有显著差别，从而也会导致分类结果有显著不同。

### - 多维缩放

### - 主成分分析

- 主成分分析PCA
- 核化主成分分析KPCA

### - 流形学习

- 流形学习(manifold learning)是一类借鉴了拓扑流形概念的降维方法。“流 形”是在局部与欧氏空间同胚的空间，换言之，它在局部具有欧氏空间的性质， 能用欧氏距离来进行距离计算。
- 若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常 复杂，但在局部上仍具有欧氏空间的性质，因此，可以容易地在局部建立降维 映射关系，然后再设法将局部映射关系推广到全局。
- 当维数被降至二维或三维时，能对数据进行可视化展示，因此流形学习也可被 用于可视化。

### - 度量学习

- 在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空 间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在 样本属性上定义的一个距离度量，而寻找合适的空间，实质上就是在寻找一个 合适的距离度量。那么，为何不直接尝试“学习”出一个合适的距离度量呢？